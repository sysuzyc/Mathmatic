#共轭梯度法

## 1、理论说明

考虑线性方程组 :	$$Ax = b$$  ,其中，A为n阶对称正定矩阵，b是给定的n维向量。则，可以使用共轭梯度发求解得到x，算法如下所示：

<img src="https://raw.githubusercontent.com/sysuzyc/Picture/master/1646935428.jpg" width = "50%" height = "50%">



## 2、实验模型

### 2.1、 构建{A,b}

我们首先需要构建满足共轭梯度法的线性方程组。

我们使用如下代码随机生成一个对称正定方程组:

```
dimention = 100;

for i = 1:dimention

    for j = i:dimention

        temp = rand;

        A(i,j) = round(10 * temp);

        A(j,i) = A(i,j);

    end

end

for i = 1 : dimention

    A(i,i) = 100;

end
```

 上面代码生成的是一个100维的对称正定矩阵。现在，我们来验证下该矩阵是否是正定矩阵。

```
[V,D] = eig(A);
Matrix_valid = 1;
for i = 1:dimention
    if (D(i,i) < 0) 
        Matrix_valid = 0;
    end
end
```

我们计算了矩阵A的特征值，如果矩阵A的所有特征值都为正数，则该矩阵为正定矩阵。

因此，我们可以说，构建了一个对称正定矩阵A。

接下来，我们使用如下方法构建n维向量：

```
for i = 1:dimention
    b(i,1) = 10*rand;
end
```

我们这里得到的是[0,10]之间的随机数构成的n维向量。

因此，我们需要做的就是计算 $$Ax = b$$ 。

### 2.2、共轭梯度法

```
% 初始化数据
x0 = zeros(dimention,1);
x(:,1) = x0;
r(:,1) = b - A*x(:,1);
k = 1;
%共轭梯度法核心算法
model(k) = 100; 
while(model(k) > 10^-9)
    k = k + 1
    if k == 2
        p(:,1) = r(:,1);
    else
        belta(:,k-2) = (r(:,k-1)' * r(:,k-1)) / (r(:,k-2)' * r(:,k-2));
        p(:,k-1) = r(:,k-1) + belta(:,k-2) * p(:,k-2);
    end
    alfa(:,k-1) = (r(:,k-1)' * r(:,k-1)) / (p(:,k-1)' * A * p(:,k-1));
    x(:,k) = x(:,k-1) + alfa(:,k-1) * p(:,k-1);
    r(:,k) = r(:,k-1) - alfa(:,k-1) * A * p(:,k-1);
    model(k) = norm(r(:,k));
end
x_final = x(:,k);
```

我们使用上面的代码就可以实现共轭梯度法了。

## 3、实验结果

本次实验结果一共分为两个部分说明，首先是针对于共轭梯度法在不同的参数约束下的收敛次数的比较，其次是将共轭梯度法和最速下降法来进行比较。

### 3.1、共轭梯度法

我们为了能够确保输入的数据是一致的，我们保存随机生成的一个对称正定矩阵A和n维向量。具体数据可以在文末查看。

这里，对于算法的精度进行探讨，主要获取的是两个数据：

- 共轭梯度法的迭代次数
- 收敛后的残差矩阵的模长


|  精确度  | 迭代次数 |       残差模长        |
| :------: | :------: | :-------------------: |
| $$e^-3$$ |    11    | 4.739767487550580e-04 |
| $$e^-4$$ |    13    | 4.139875743521754e-05 |
| $$e^-5$$ |    15    | 5.621490749686395e-06 |
| $$e^-6$$ |    18    | 3.760057264014553e-07 |
| $$e^-7$$ |    20    | 3.223850333724713e-08 |
| $$e^-8$$ |    22    | 2.862697616714050e-09 |
| $$e^-9$$ |    23    | 8.426950350018659e-10 |

从上面我们可以看出，对于100维的对称正定矩阵进行共轭梯度法求解，迭代次数是随着收敛的精度而增加的，但是在增长到$$e^-8$$ 之后的增长幅度就降下来了，并且这个时候的精度也足够我们使用了。

因此，在使用共轭梯度法的时候，计算时间和计算精度我们需要做一个平衡，才能得到最优的结果。



### 3.2、共轭梯度法与最速下降法

我们这里使用的是同样的数据，实验方法同上，通过变动精确度来进行结果的对比。

|  精确度  | CG迭代次数 |      CG残差模长       | FG迭代次数 |      FG残差模长       |
| :------: | :--------: | :-------------------: | :--------: | :-------------------: |
| $$e^-3$$ |     11     | 4.739767487550580e-04 |     54     | 7.749837721692668e-04 |
| $$e^-4$$ |     13     | 4.139875743521754e-05 |     66     | 9.524598292941505e-05 |
| $$e^-5$$ |     15     | 5.621490749686395e-06 |     80     | 8.312927183513835e-06 |
| $$e^-6$$ |     18     | 3.760057264014553e-07 |     94     | 7.290258600410878e-07 |
| $$e^-7$$ |     20     | 3.223850333724713e-08 |    106     | 9.077847493482585e-08 |
| $$e^-8$$ |     22     | 2.862697616714050e-09 |    120     | 8.012108410120650e-09 |
| $$e^-9$$ |     23     | 8.426950350018659e-10 |    134     | 7.095130124215742e-10 |

从上面，我们可以看出在相同精确度的情况下共轭梯度法(CG)会比最速下降法(FG)的迭代次数更少，最终的残差也是更小。从这个角度可以看出，共轭梯度法是优于最速下降法的。

## 4、实验数据及代码

本次实验的代码是在MATLAB下实现的，主要分为三个文件：

main.m , CG.m , FG.m

其中主函数main.m为：

```
load('A.mat');
load('b.mat');
%[k,x_final] = CG(A,b,dimention);
[k,x_final] = FG(A,b,dimention);
%计算残差矩阵的模长
error = norm(b - A * x_final);
```

共轭梯度法部分CG.m为：

```
function [k,x_final] = CG(A,b,dimention)
% 初始化数据
x0 = zeros(dimention,1);
x(:,1) = x0;
r(:,1) = b - A*x(:,1);
k = 1;
%共轭梯度法核心算法
model(k) = 100; 
while(model(k) > 10^-9)
    k = k + 1
    if k == 2
        p(:,1) = r(:,1);
    else
        belta(:,k-2) = (r(:,k-1)' * r(:,k-1)) / (r(:,k-2)' * r(:,k-2));
        p(:,k-1) = r(:,k-1) + belta(:,k-2) * p(:,k-2);
    end
    alfa(:,k-1) = (r(:,k-1)' * r(:,k-1)) / (p(:,k-1)' * A * p(:,k-1));
    x(:,k) = x(:,k-1) + alfa(:,k-1) * p(:,k-1);
    r(:,k) = r(:,k-1) - alfa(:,k-1) * A * p(:,k-1);
    model(k) = norm(r(:,k));
end
x_final = x(:,k);

end
```

最速下降法部分FG.m为：

```
function [k,x_final] = FG(A,b,dimention)
x0 = zeros(dimention,1);
x(:,1) = x0;
r(:,1) = b - A*x(:,1);
k = 1;
%最速下降法核心算法
model(k) = 100; 
while(model(k) > 10^-9)
    k = k + 1
    alfa(:,k-1) = r(:,k-1)' * r(:,k-1) / (r(:,k-1)' * A * r(:,k-1));
    x(:,k) = x(:,k-1) + alfa(:,k-1) * r(:,k-1);
    r(:,k) = b - A * x(:,k)
    model(k) = norm(r(:,k));
end
x_final = x(:,k);
end
```

实验的数据托管在github上，可直接点击相应名称获取对应数据：

[A.mat](https://github.com/sysuzyc/Mathmatic/blob/master/CG/A.mat)  ,  [b.mat](https://github.com/sysuzyc/Mathmatic/blob/master/CG/b.mat) 











